{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classifier for Predicting Smoker Status \n\nCan we predict whether someone has been a smoker (Has Quit or Currently is) with biometric data?\nI'm not sure, but let's find out.\n\nWe'll start with the smoking-drinking-dataset available here (automatically loaded to the workspace if you are viewing this on Kaggle):\nhttps://www.kaggle.com/datasets/sooyoungher/smoking-drinking-dataset\n\nThis dataset, contributed by Kaggler Soo.Y, was collected from National Health Insurance Service in Korea. The dataset has 991,000 labeled observations and seems very clean. \n\nNext, we'll need some libraries so we can load the data and run the analysis.\n","metadata":{}},{"cell_type":"code","source":" \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import PrecisionRecallDisplay\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-09T08:36:30.680370Z","iopub.execute_input":"2023-09-09T08:36:30.680942Z","iopub.status.idle":"2023-09-09T08:36:32.683400Z","shell.execute_reply.started":"2023-09-09T08:36:30.680897Z","shell.execute_reply":"2023-09-09T08:36:32.682090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read in the data to a dataframe\ndf = pd.read_csv('/kaggle/input/smoking-drinking-dataset/smoking_driking_dataset_Ver01.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:36:32.685446Z","iopub.execute_input":"2023-09-09T08:36:32.686051Z","iopub.status.idle":"2023-09-09T08:36:38.371293Z","shell.execute_reply.started":"2023-09-09T08:36:32.686011Z","shell.execute_reply":"2023-09-09T08:36:38.370103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape\",df.shape)\nprint(df.describe())","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:36:38.372460Z","iopub.execute_input":"2023-09-09T08:36:38.372809Z","iopub.status.idle":"2023-09-09T08:36:39.384750Z","shell.execute_reply.started":"2023-09-09T08:36:38.372780Z","shell.execute_reply":"2023-09-09T08:36:39.383471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA - Exploratory Data Analysis\nThis data is pretty clean (No minimums less than 1), and sizeable (991346 rows of data).\n\n## Preprocessing and Data Cleansing\nAre there any categorical variables? Let's look at a row to see.\n","metadata":{}},{"cell_type":"code","source":"df.iloc[0,:]","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:36:39.386917Z","iopub.execute_input":"2023-09-09T08:36:39.387220Z","iopub.status.idle":"2023-09-09T08:36:39.395188Z","shell.execute_reply.started":"2023-09-09T08:36:39.387193Z","shell.execute_reply":"2023-09-09T08:36:39.394184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems we have two binary categorical variables. \n- Sex (F/M) \n- Drinker (yes/no)\n\nBoth are binary so let's encode those variables with True/False for 1s and 0s later.\n\nAt the same time, let's address our target variables of Smoking.\n- SMK_stat_type_cd = Smoking state, 1(never), 2(used to smoke but quit), 3(still smoke)\n- We'll encode this as 0 never smoked, 1 has or is a smoker","metadata":{}},{"cell_type":"code","source":"df[\"female\"] = pd.get_dummies(df['sex'],1).iloc[:,0]\n\nfor i in range(df.shape[0]):\n    if df['female'][i] == False:\n        df['female'][i] = 0\n    else:\n        df['female'][i] = 1\n        \ndf.iloc[:,[0,-1]]","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:36:39.396332Z","iopub.execute_input":"2023-09-09T08:36:39.396957Z","iopub.status.idle":"2023-09-09T08:37:04.886413Z","shell.execute_reply.started":"2023-09-09T08:36:39.396905Z","shell.execute_reply":"2023-09-09T08:37:04.885356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"drinker\"] = pd.get_dummies(df['DRK_YN'],0).iloc[:,1]\nfor i in range(df.shape[0]):\n    if df['drinker'][i] == False:\n        df['drinker'][i] = 0\n    else:\n        df['drinker'][i] = 1\n        \n\ndf.iloc[:,[-3,-1]]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:37:04.887612Z","iopub.execute_input":"2023-09-09T08:37:04.887980Z","iopub.status.idle":"2023-09-09T08:37:30.760617Z","shell.execute_reply.started":"2023-09-09T08:37:04.887950Z","shell.execute_reply":"2023-09-09T08:37:30.759523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['smokers'] = df['SMK_stat_type_cd'] - 1 #sets ones to zero, twos to ones, and threes to twos\ndf['smokers'].loc[df['smokers'] > 1  ] = 1 #changes twos to ones\ndf['smokers'].describe() #mean shows us how many smokers in dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:37:30.762209Z","iopub.execute_input":"2023-09-09T08:37:30.762551Z","iopub.status.idle":"2023-09-09T08:37:30.826574Z","shell.execute_reply.started":"2023-09-09T08:37:30.762523Z","shell.execute_reply":"2023-09-09T08:37:30.825485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for NaNs\nfor i in df.columns:\n    print(i,\":\",df[i].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:37:30.827776Z","iopub.execute_input":"2023-09-09T08:37:30.828134Z","iopub.status.idle":"2023-09-09T08:37:31.282812Z","shell.execute_reply.started":"2023-09-09T08:37:30.828102Z","shell.execute_reply":"2023-09-09T08:37:31.281471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('SMK_stat_type_cd', axis=1, inplace=True)\ndf.drop(\"DRK_YN\", axis=1, inplace=True)\ndf.drop(\"sex\", axis=1, inplace =True)\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:37:31.286202Z","iopub.execute_input":"2023-09-09T08:37:31.286580Z","iopub.status.idle":"2023-09-09T08:37:31.678035Z","shell.execute_reply.started":"2023-09-09T08:37:31.286551Z","shell.execute_reply":"2023-09-09T08:37:31.676789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have all numerical data, a coded target variable, and cleaned up dataframe without NaNs.\n\nOur dataset isn't totally balanced, we see that 39.2% of respondents are or were smokers, and the remainder never started.\n\n## Visualize Correlations\nLet's move onto visualize and model the data. First we'll run a correlation plot to see how bad the multicollinearity is on this data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(df.corr(method='pearson'), annot=True)\nheatmap.set_title('Pairwise Correlation Heatmap', fontdict={'fontsize':6}, pad=12);","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:37:31.679927Z","iopub.execute_input":"2023-09-09T08:37:31.680418Z","iopub.status.idle":"2023-09-09T08:37:36.180432Z","shell.execute_reply.started":"2023-09-09T08:37:31.680379Z","shell.execute_reply":"2023-09-09T08:37:36.179345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, \n- there are some highly correlated columns depending up a person's sex.\n- height is highly correlated to weight.\n- waistline is highly correlated to weight.\n- Systolic Blood Pressures is highly correlated to Diastolic Blood Pressure.\n- LDL Cholestoral is highly correlated to total cholestoral.\n- SGOT_AST(Glutamate-oxaloacetate transaminase)(Aspartate transaminase) correlates highly with (Alanine transaminase).\n\nThis is expected in all but the sex category. \n- The sex of a person is descriptive. \n- The rest of these are examples of measuring similar attributes differently.\n\nThis presents options: \n1. We'll have to progress through forward and backward selection to see which ones present collinearity issues in a linear regression\n2. Or we can see if decision trees do a good job with the data first \n- A: Base trees will be prone to overfitting\n- B: ADA Boost will be better than Base Trees\n- C: Random Forest will be able to handle the collinearity and pull patterns across collinearity.\n\nLet's try # 2C first.\n\n## Build & Train a Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"Xtr,Xte, ytr, yte = train_test_split(df.iloc[:,0:-2],df.iloc[:,-1],test_size=0.2,shuffle=True,random_state =12)\nprint(Xtr.head())\n\nprint(\"Shapes (train):\",Xtr.shape,ytr.shape)\nprint(\"Shapes (test):\", Xte.shape,yte.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:37:36.181776Z","iopub.execute_input":"2023-09-09T08:37:36.182235Z","iopub.status.idle":"2023-09-09T08:37:36.815388Z","shell.execute_reply.started":"2023-09-09T08:37:36.182198Z","shell.execute_reply":"2023-09-09T08:37:36.814168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For documentation sake, what are the Random Forest Model parameters?","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 12)\n\nfrom pprint import pprint\n\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:37:36.816717Z","iopub.execute_input":"2023-09-09T08:37:36.817196Z","iopub.status.idle":"2023-09-09T08:37:36.825218Z","shell.execute_reply.started":"2023-09-09T08:37:36.817151Z","shell.execute_reply":"2023-09-09T08:37:36.823806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, a few things stay a few things to change. Gini Coefficient is good and will stay (detecting purity of the splits) and a square root of the number of features will help the classifier mitigate the collinearity. \n\nMoving on to things we'll change, we want to use stumps (max depth 2), expect a few samples per leaf (we'll say 12 to start), want to run the job in parallel (n_jobs = -1), want to bootstrap the data for randomization (so central limit theorem helps us avoid overfitting), and a whole bunch of trees (say 1200). \n\nWe set those parameters, train the model with the training data, and then predict both the test and train outputs with our new model. Verbose is set to 1 so we will get to see some of the progress.","metadata":{}},{"cell_type":"code","source":"class1rf = RandomForestClassifier(n_estimators=212,  criterion='gini',max_features='sqrt',\n                                  bootstrap=True, max_depth=2, min_samples_leaf = 12, n_jobs=-1,\n                                 verbose=1, ccp_alpha = .00) \n\n#fit the classifier to training data\nclass1rf.fit(Xtr,ytr)                                  \nprint(\"Random Forest fit. \\n Predicting Train Values...\" )\n\n#predict the training dataset\nyarf = class1rf.predict(Xtr)\n\n#predict the test dataset\nyarf2 = class1rf.predict(Xte)                              ","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:59:14.482246Z","iopub.execute_input":"2023-09-09T10:59:14.482675Z","iopub.status.idle":"2023-09-09T11:00:05.506860Z","shell.execute_reply.started":"2023-09-09T10:59:14.482638Z","shell.execute_reply":"2023-09-09T11:00:05.505893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Score the Initial Random Forest Classifier\nWith our fit training data, and predicted train and test set data, let's score the model.\n\n### Numerical Scores","metadata":{}},{"cell_type":"code","source":"C1Trroc = roc_auc_score(ytr,yarf)\nC1Teroc = roc_auc_score(yte,yarf2)\n\nC1Trps = precision_score(ytr,yarf)\nC1Teps = precision_score(yte,yarf2)\n\nC1Trrs = recall_score(ytr,yarf)\nC1Ters = recall_score(yte,yarf2)\n\nC1Trf1 = 2*(C1Trrs * C1Trps)/(C1Trrs + C1Trps)\nC1Tef1 =  2*(C1Ters * C1Teps)/(C1Ters + C1Teps)\n\nscoring = pd.DataFrame({\n             \"model\" : [\"Train\", \"Test\"],\n             \"Precision\" : [C1Trps,C1Teps],\n             \"Recall\" : [C1Trrs,C1Ters],\n             \"F1 Score\" : [C1Trf1,C1Tef1],\n             \"ROC-AUC\": [C1Trroc,C1Teroc]\n             \n             })\nscoring ","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:38:42.113599Z","iopub.execute_input":"2023-09-09T08:38:42.113958Z","iopub.status.idle":"2023-09-09T08:38:43.766694Z","shell.execute_reply.started":"2023-09-09T08:38:42.113929Z","shell.execute_reply":"2023-09-09T08:38:43.765497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visual Score Graphics","metadata":{}},{"cell_type":"code","source":"C1Trfp,C1Trtp, _ = roc_curve(ytr,yarf)\nC1Tefp,C1Tetp, _ = roc_curve(yte,yarf2)\n#Training ROC\nplt.figure(figsize=[8,5])\nplt.subplot(121)\nplt.fill_between(C1Trfp,C1Trtp)\nplt.ylabel(\"True Positive\")\nplt.xlabel(\"False Positive\")\nplt.title(\"Train ROC Curve\");\n\nplt.subplot(122)\n#Testing ROC \nplt.fill_between(C1Tefp,C1Tetp)\n\nplt.xlabel(\"False Positive\")\nplt.title(\"Test ROC Curve\");\nplt.show()\nprint(\"Train AUC:\", C1Trroc ,\"\\t \\t Test AUC:\" , C1Teroc )","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:38:43.768207Z","iopub.execute_input":"2023-09-09T08:38:43.769267Z","iopub.status.idle":"2023-09-09T08:38:44.365775Z","shell.execute_reply.started":"2023-09-09T08:38:43.769227Z","shell.execute_reply":"2023-09-09T08:38:44.364850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"C1Trprec,C1Trrecall, _ = precision_recall_curve(ytr,yarf)\nC1Teprec,C1Terecall, _ = precision_recall_curve(yte,yarf2)\n#Training Precision Recall Curve\nplt.figure(figsize=[8,5])\nplt.subplot(121)\nplt.fill_between(C1Trrecall,C1Trprec)\nplt.ylabel(\"Precision\")\nplt.xlabel(\"Recall\")\nplt.title(\"Train Precision-Recall curve\");\n\nplt.subplot(122)\n#Training Precision Recall Curve\nplt.fill_between(C1Trrecall,C1Trprec)\n#plt.ylabel(\"Precision\")\nplt.xlabel(\"Recall\")\nplt.title(\"Test Precision-Recall curve\");\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:38:44.366967Z","iopub.execute_input":"2023-09-09T08:38:44.368036Z","iopub.status.idle":"2023-09-09T08:38:44.968249Z","shell.execute_reply.started":"2023-09-09T08:38:44.368001Z","shell.execute_reply":"2023-09-09T08:38:44.966906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial Classifier Analysis\nThese results aren't bad, we are over 81% on recall, 70% on precision, and have F1 scores in the .75 area for both train and test sets. This tells us we have probably not overfit our model because the metrics are similar whether the model has seen the data (training set) or is new data (test set). Moreover, area under the ROC-AUC and Precision Recall Curves appears to illustrate identical shapes and more area identified than blank. We have succussfully fit a model in under 10 minutes, tested, scored, and completed our cursory analysis. \n\nHowever, we haven't tuned this model at all. The hyperparameters we chose were just guesses. The cursory analysis does show that this method is promising, but to deploy a model like this we'd like better scores. Therefore, the value of the cursory model justifies investing time to tune the model. Let's see if we can increase those scores by tuning the hyperparameters.\n\n## Tune Random Forest Classifier Hyperparameters\nHere, we'll use the gridsearchCV method to tune our random forest.\n- Start by choosing the parameters for the grid.\n- Train the data on each of the sets of parameters.\n- Select the best set of parameters.\n- Predict train outcomes and then test outcomes.\n- Run metrics on those outcomes to gauge efficacy, overfitting, and final recommendation.\n","metadata":{}},{"cell_type":"code","source":"# Build the grid of options \ngrids = {\n        'n_estimators':[int(N) for N in np.linspace(100,1000,3)],\n        'max_depth': [2,3,4,5]\n        #'min_samples_split': [.1,.01,.05],\n        #'min_samples_leaf': [2,27,52,104],   \n        #'max_samples' : [ int(N) for N in np.linspace(1000,12000,6)] \n        }\n# Let people see the output of this step...\ngrids","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:38:44.972500Z","iopub.execute_input":"2023-09-09T08:38:44.973282Z","iopub.status.idle":"2023-09-09T08:38:44.983591Z","shell.execute_reply.started":"2023-09-09T08:38:44.973228Z","shell.execute_reply":"2023-09-09T08:38:44.981984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run grid search on random forest classifier, with grid options, \n# on a single proc (the other 3 are for RandomForest to run full blast),\n# with 3 fold validation, verbose output (3), scoring based on f1 score,\n# and raise an error if score is nan.\nhypertesting = GridSearchCV(class1rf, \n                            param_grid=grids, n_jobs=1, \n                            cv= 3, verbose = 3, scoring='f1',\n                            error_score ='raise'  ).fit(Xtr,ytr)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T08:39:29.062212Z","iopub.execute_input":"2023-09-09T08:39:29.062678Z","iopub.status.idle":"2023-09-09T10:07:28.636847Z","shell.execute_reply.started":"2023-09-09T08:39:29.062630Z","shell.execute_reply":"2023-09-09T10:07:28.635097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Table of F1 Scores and Parameters","metadata":{}},{"cell_type":"code","source":"seee = pd.DataFrame(hypertesting.cv_results_)\nseee","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:07:28.639523Z","iopub.execute_input":"2023-09-09T10:07:28.640378Z","iopub.status.idle":"2023-09-09T10:07:28.674237Z","shell.execute_reply.started":"2023-09-09T10:07:28.640327Z","shell.execute_reply":"2023-09-09T10:07:28.672832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combine Scores With Initial Classifer Results","metadata":{}},{"cell_type":"code","source":"#predict the test set with optimized model\npreds = hypertesting.predict(Xte)\n \n#score the test set (ROC-AUC, Precision, Recall, & F1)\nC2Teroc = roc_auc_score(yte,preds)\n \nC2Teps = precision_score(yte,preds)\n \nC2Ters = recall_score(yte,preds)\n \nC2Tef1 =  2*(C2Ters * C2Teps)/(C2Ters + C2Teps)\n\n#add scores to dataframe of metrics above for analysis\nscoring.loc[len(scoring.index)] = ['GridSearch + Test Set', C2Teps, C2Ters, C2Tef1,C2Teroc] \nscoring","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:38:58.609229Z","iopub.execute_input":"2023-09-09T10:38:58.609657Z","iopub.status.idle":"2023-09-09T10:39:02.911038Z","shell.execute_reply.started":"2023-09-09T10:38:58.609624Z","shell.execute_reply":"2023-09-09T10:39:02.909913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis","metadata":{}},{"cell_type":"markdown","source":"The GridSearch returned a best F1 score of 78.4% on the training data. The dataframe above shows that the F1 Score on the unseen test set is even higher at 78.53%. This leads us to believe that the model is not overfit. \n\nAdditionally, the optimized model results in a 2.5% increase in F1 scores versus the untuned model (76% on test set vs 78.5% on test set). This is a substantial increase, and warrants a look under the hood. Further analysis shows that the Recall of the tuned model increased 7 points, and the Precision stayed roughly the same (a loss of .5% in precision). The extra hour of processing time to run the grid search did increase the value of the model.\n\n### Visual Score Graphics\nBefore we go, we'll generate the ROC and Precision-Recall curves of the intial testset results and the final test set results.","metadata":{}},{"cell_type":"code","source":"C2Tefp,C2Tetp, _ = roc_curve(yte,preds) \n\nplt.figure(figsize=[8,5])\nplt.subplot(121)\n#Initial Test ROC \nplt.fill_between(C1Tefp,C1Tetp)\n\nplt.xlabel(\"False Positive\")\nplt.title(\"Test ROC Curve\");\n\n#Grid Test ROC\nplt.subplot(122)\nplt.fill_between(C2Tefp,C2Tetp)\nplt.ylabel(\"True Positive\")\nplt.xlabel(\"False Positive\")\nplt.title(\"Grid Test ROC Curve\");\n\n\nplt.show()\nprint(\"Test AUC:\", C1Teroc ,\"\\t \\t GridSearch Test AUC:\" , C2Teroc )\n\nC2Teprec,C2Terecall, _ = precision_recall_curve(yte, preds) \n\n \nplt.figure(figsize=[8,5])\nplt.subplot(121)\n#Initial Testing Precision Recall Curve\nplt.fill_between(C1Trrecall,C1Trprec)\n#plt.ylabel(\"Precision\")\nplt.xlabel(\"Recall\")\nplt.title(\"Test Precision-Recall curve\")\n\n\nplt.subplot(122)\n# Final Testing Precision Recall Curve\nplt.fill_between(C2Terecall,C2Teprec)\nplt.ylabel(\"Precision\")\nplt.xlabel(\"Recall\")\nplt.title(\"Grid Search Test Precision-Recall curve\");\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-09T10:56:56.773578Z","iopub.execute_input":"2023-09-09T10:56:56.774155Z","iopub.status.idle":"2023-09-09T10:56:57.712903Z","shell.execute_reply.started":"2023-09-09T10:56:56.774108Z","shell.execute_reply":"2023-09-09T10:56:57.711808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Analysis of Classifier\nYes, there is appreciable improvment in the output graphics for ROC and Precision-Recall. But they are certainly not earth shattering. I would agree that there is more tuning to be done. We could certainly continue to tune this model. Viable reasons include a competition or because a client was willing to pay for it. However, the initial grid search tells us that we have reached diminishing returns on this particular model. Without feature engineering, different data, or other manipulation of the setup, this model is probably within 10% of the final results. \n\nHere, we revert to the initial research question, and deliver our results.\n\n# Answer Initial Research Inquiry\nCan we determine if a person was a smoker based on this dataset?\n- The answer is yes, with a 90% recall rate and 69% precision on those predictions. \n- In other words our model can find 9/10 of the smokers, and is accurate in 7/10 predictions.\n\nThe only thing left to determine are the indicators the model is using for these predictions. \n\nHow is the model making these decisions?\n\n# Explain the Model\nWe'll use the shapely values to illustrate how the model thinks on three random cases. \n- First we'll build the explainer on the gridsearch best model.\n- Then we'll output a graphic representation (forceplot) of why the model returned the probability for each case.\n- And finally check whether the model categorized each case correctly.\n\n## Granular Model Decision Analysis \n### Shap Force Plot1 - Observation 312","metadata":{}},{"cell_type":"code","source":"import shap  \n# Create Tree Explainer object that can calculate shap values\nexplainer = shap.TreeExplainer(hypertesting.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:10:24.996142Z","iopub.execute_input":"2023-09-09T11:10:24.996643Z","iopub.status.idle":"2023-09-09T11:10:30.158282Z","shell.execute_reply.started":"2023-09-09T11:10:24.996582Z","shell.execute_reply":"2023-09-09T11:10:30.157290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Shap values on a random instance\nchosen_instance = Xte.iloc[312,:]\nshap_values = explainer.shap_values(chosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], chosen_instance)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:20:42.332724Z","iopub.execute_input":"2023-09-09T11:20:42.333150Z","iopub.status.idle":"2023-09-09T11:20:42.354898Z","shell.execute_reply.started":"2023-09-09T11:20:42.333120Z","shell.execute_reply":"2023-09-09T11:20:42.353887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that the most significant factor in the non-smoking classification is that the subject is female. Hemoglobin, height, weight, serum creatine score, and waistline size also contribute to the 0 class score. However, triglyceride levels and gamma_GTP scores keep this classification from being a perfect zero log-odds. The model log odds for a smoker are .12, which is less than the .5 decision boundary between the classes. The model classifies this as a non-smoker.\n","metadata":{}},{"cell_type":"code","source":"print(f\"The actual value from the test set for record 312 is {yte.iloc[312]}, which corroborates the model.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:36:37.438189Z","iopub.execute_input":"2023-09-09T11:36:37.438588Z","iopub.status.idle":"2023-09-09T11:36:37.444056Z","shell.execute_reply.started":"2023-09-09T11:36:37.438559Z","shell.execute_reply":"2023-09-09T11:36:37.442819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shap Force Plot2 - Observation 712","metadata":{}},{"cell_type":"code","source":"# Calculate Shap values on a random instance\nchosen_instance = Xte.iloc[712,:]\nshap_values = explainer.shap_values(chosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], chosen_instance)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:14:02.245736Z","iopub.execute_input":"2023-09-09T11:14:02.246422Z","iopub.status.idle":"2023-09-09T11:14:02.270508Z","shell.execute_reply.started":"2023-09-09T11:14:02.246380Z","shell.execute_reply":"2023-09-09T11:14:02.269530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that the most significant factor in the smoking classification is that the subject is male. Height, gamma_GTP, and triglyceride levels contribute to that score in descending importance. The age of 40 is significant, but not as much so. Hemoglobin and weight characteristics of the subject reduce the log-odds score, but not below the threshold for a decision. The model log odds for a smoker are .67, which is more than the .5 decision boundary for the model. So the model classifies this as a smoker.","metadata":{}},{"cell_type":"code","source":"print(f\"The actual value from the test set is {yte.iloc[712]}, which corroborates the model.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:40:01.734553Z","iopub.execute_input":"2023-09-09T11:40:01.735692Z","iopub.status.idle":"2023-09-09T11:40:01.741815Z","shell.execute_reply.started":"2023-09-09T11:40:01.735648Z","shell.execute_reply":"2023-09-09T11:40:01.740708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shap Force Plot3 - Observation 1212","metadata":{}},{"cell_type":"code","source":"# Calculate Shap values on a random instance\nchosen_instance = Xte.iloc[1212,:]\nshap_values = explainer.shap_values(chosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], chosen_instance)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:14:24.902365Z","iopub.execute_input":"2023-09-09T11:14:24.902807Z","iopub.status.idle":"2023-09-09T11:14:24.926332Z","shell.execute_reply.started":"2023-09-09T11:14:24.902740Z","shell.execute_reply":"2023-09-09T11:14:24.925175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, again, we see that the most significant factor in the non-smoking classification is that the subject is female. Hemoglobin, Gamma_GTP, height, serum creatine levels and weight contribute to the 0 class score in descending importance. However, hemoglobin levels keep this classification from being a perfect zero log-odds. The model log odds for a smoker are .16, which is less than the .5 decision boundary between the classes. The model classifies this as a non-smoker.\n","metadata":{}},{"cell_type":"code","source":"print(f\"The actual value from the test set is {yte.iloc[1212]}, which corroborates the model.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-09T11:42:12.027326Z","iopub.execute_input":"2023-09-09T11:42:12.028006Z","iopub.status.idle":"2023-09-09T11:42:12.033357Z","shell.execute_reply.started":"2023-09-09T11:42:12.027967Z","shell.execute_reply":"2023-09-09T11:42:12.032125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Feature Importance - Summary Plot","metadata":{}},{"cell_type":"code","source":"#run shap_values on whole test set for summary plot\nvals = explainer.shap_values(Xte)\nshap.summary_plot(vals,  max_display = 22, feature_names=Xte.columns)","metadata":{"execution":{"iopub.status.busy":"2023-09-09T13:10:57.611179Z","iopub.execute_input":"2023-09-09T13:10:57.611685Z","iopub.status.idle":"2023-09-09T13:20:55.518411Z","shell.execute_reply.started":"2023-09-09T13:10:57.611648Z","shell.execute_reply":"2023-09-09T13:20:55.517270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The summary plot above tells us which features have the most importance in the model (gender is greatest). Since the Random Forest of decision trees has many contradictory decision points (based on the values in adjacent features for any given observation) this type of plot is one of the most powerful ways of delineating what features contribute heavily to a classification of 1 or 0.\n\n# Conclusion \nWe can see who is or was a smoker based on their vital signs with well better than 50% guessing accuracy, (69.4%) and we can find 90% of those smokers with the model. This is good. \n\nHowever, this model doesn't have a high enough F1 score (over 80% precision and recall) to prescribe outcomes in an automated fashion.\n\nThanks for taking a moment to look at this notebook.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}